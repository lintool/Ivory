<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>Ivory: Preprocessing and Indexing Pipeline</title>
<style type="text/css" media="screen">@import url( style.css );</style>
</head>

<body>

<div id="wrap">
<div id="container" class="one-column" >

<!-- header START -->

<div id="header">
<div id="caption">
<h1 id="title" style="color: white;">Ivory</h1>
<div id="tagline">A Hadoop toolkit for web-scale information retrieval research</div>
</div>

<div class="fixed"></div>

</div>

<!-- header END -->

<!-- navigation START -->

<div id="navigation">
<ul id="menus">

  <li class="page_item"><a class="home" title="Home" href="../index.html">Home</a></li>
  <li class="page_item"><a href="api/index.html" title="API">API</a></li>
  <li class="page_item"><a href="publications.html" title="Publications">Publications</a></li>
  <li class="page_item"><a href="regression.html" title="Experiments">Experiments</a></li>
  <li class="page_item"><a href="team.html" title="Team">Team</a></li>

</ul>

<div class="fixed"></div>

</div>

<!-- navigation END -->



<!-- content START -->

<div id="content">



	<!-- main START -->

	<div id="main">


<!--- START MAIN CONTENT HERE -->

<h2>Ivory: Bilingual Text Classification</h2>

<div class="post">
<div class="content">

<p>
Bilingual text, or bitext (also called parallel corpus) consists
of aligned sentence pairs (<i>f</i>,<i>e</i>) where sentences 
<i>f</i> and <i>e</i> are written in different languages, yet 
have the same meaning. The task of the bitext classifier is 
to determine if a given pair should be aligned.

There have been many approaches to this problem in the literature,
and the most useful features have consistently been length-based
and token alignment-based features. Below is a list of features we
consider and their detailed descriptions.

<ol>

<li><b>Cosine similarity:</b> A popular similarity measure, defined
as the cosine of the angle between two vector representations. In 
our case, we represent sentences <i>e</i> and <i>f</i> as a vector 
of term weights. The vector representing f is translated into the
vocabulary of e's language using cross-language information retrieval
(CLIR) techniques based on Darwish and Oard's paper (2004).
For term weighting, we use Okapi BM25 (k<sub><small>1</small></sub>=1.2, 
b=0.75) in our experiments, although any term weighting scheme is possible.
</li>

<li>
<b>Length ratio:</b> The ratio of sentence lengths (i.e., number of tokens).
</li>

<li><b>Token translation ratio:</b> The ratio of tokens in <i>f</i>
that have translations in <i>e</i>, and vice versa. It is possible
to enforce a lower probability limit <i>L</i> to the translations of a token 
<i>f</i><sub><small>i</small></sub>, by only considering tokens 
<i>e</i><sub><small>j</small></sub> such that 
<i>t</i>(e<sub><small>j</small></sub>|f<sub><small>i</small></sub>)><i>L</i>.
</li>

<li><b>Uppercase translation ratio:</b>  The ratio of upper-cased entities in 
<i>f</i> that also appear in <i>e</i>. We define an upper-cased entity as a
multi-token span of consecutive upper-cased tokens, such that tokens to the left and
right of the entity are lower-cased.
</li>
</ol>

The pipeline for training and testing a bitext classifier is handled by the
class ivory.lsh.eval.BitextClassifierUtils, which relies on core Ivory classes
to perform tokenization (ivory.core.tokenize.*), represent
token translation probabilities and vocabularies (edu.umd.hooka.*), 
and translate vectors (ivory.core.util.CLIRUtils). 
See documentation for Hooka
(released as part of <a href="http://lintool.github.com/Cloud9">Cloud9</a>)
to learn more about how token translation tables and vocabularies are represented
and built. See <a href="publications/Ture_etal_SIGIR2011.pdf">our SIGIR 2011 paper</a> for
a detailed description of our CLIR-based translation approach.

<p>Below is the pipeline of how a given sentence pair is classified:</p>

<p><img style="margin-left: 50px;" width="100%" src="images/bitext.png" alt="Maxent Classification Pipeline" /></p>

<p>The maximum-entropy classifier needs to be trained before we can classify any
sentence pair. This is done by running a set of labeled sentence pairs through 
this pipeline, and have the maximum-entropy learner optimize parameters iteratively:</p>

<p><img style="margin-left: 50px;" width="70%" src="images/maxent.png" alt="Maxent Training Pipeline" /></p>

The open-source OpenNLP MaxEnt toolkit (v3.0.0) is used to learn and apply maximum-entropy classification.
For training purposes, we sample 1,000 sentence pairs from an available parallel corpus to serve as positive
instances. In order to generate negative instances, we randomly pick 5,000 sentence pairs from possible 
mis-aligned sentence pairs in the corpus (e.g., 7th source-language sentence and 3rd target-language sentence).
We do not use all 999,000 possible negative instances for training in order to keep a reasonable positive-negative
instance ratio and avoid a biased learning process.

<b>
<p>increasing training data</p>

<p>compare feature sets</p>

<p>vary L = 0,0.1,...,0.5</p>

<p>different domain?</p>
</b>

<p style="padding-top: 25px"><a href="../index.html">Back to main page</a></p>

</div></div>

<!--- END MAIN CONTENT HERE -->

	</div>

	<!-- main END -->



		<div class="fixed"></div>

</div>

<!-- content END -->

<!-- footer START -->

<div id="footer">
<div id="copyright">
Last updated:
<script type="text/javascript">
<!--//
document.write(document.lastModified);
//-->
</script>
</div>

<div id="themeinfo">
Adapted from a WordPress Theme by <a href="http://www.neoease.com/">NeoEase</a>. Valid <a href="http://validator.w3.org/check?uri=referer">XHTML 1.1</a> and <a href="http://jigsaw.w3.org/css-validator/check/referer?profile=css3">CSS 3</a>.	</div>

</div>

<!-- footer END -->



</div>

<!-- container END -->

</div>

<!-- wrap END -->

</body>
</html>
