<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Ivory: Scalable Bitext Classification</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="assets/css/bootstrap.css" rel="stylesheet">
    <link href="assets/css/bootstrap-responsive.css" rel="stylesheet">
    <link href="assets/css/docs.css" rel="stylesheet">
    <link href="assets/js/google-code-prettify/prettify.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

  </head>

  <body data-spy="scroll" data-target=".bs-docs-sidebar">

    <!-- Navbar
    ================================================== -->
    <div class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container">
          <button type="button" class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <div class="nav-collapse collapse">
            <ul class="nav">
              <li class="">
                <a href="../index.html">Home</a>
              </li>
              <li class="active">
                <a href="./start.html">Getting Started</a>
              </li>
              <li class="">
                <a href="./publications.html">Publications</a>
              </li>
              <li class="">
                <a href="./experiments.html">Experiments</a>
              </li>
              <li class="">
                <a href="./team.html">Team</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>

<!-- Subhead
================================================== -->
<header class="jumbotron subhead" id="overview">
  <div class="container">
    <h1>Ivory</h1>
    <p class="lead">A Hadoop toolkit for web-scale information retrieval research</p>
  </div>
</header>

  <div class="container">

<div class="page-header">
<h2>Scalable Bitext Classification</h2>
<h4>Features and Implementation</h4>

</div>

<p>
Bilingual text, or bitext (also called parallel corpus) consists
of aligned sentence pairs (<i>f</i>,<i>e</i>) where sentences 
<i>f</i> and <i>e</i> are written in different languages, yet 
have the same meaning. The task of the bitext classifier is 
to determine if a given pair should be aligned. This document
describes the details of our implementation for bitext 
classification in Ivory. If you want to use our trained models
to perform bitext classification, see the 
<a href=docs/bitext-tutorial.html>step-by-step tutorial</a>.
<p>
There have been many approaches to this problem in the literature,
and the most useful features have consistently been length-based
and token alignment-based features. Below is a list of features we
consider and their detailed descriptions.
<p>

<ol>

<li><b>Cosine similarity:</b> A popular similarity measure, defined
as the cosine of the angle between two vector representations. In 
our case, we represent sentences <i>e</i> and <i>f</i> as a vector 
of term weights. The vector representing f is translated into the
vocabulary of e's language using cross-language information retrieval
(CLIR) techniques based on Darwish and Oard's paper (2004).
For term weighting, we use Okapi BM25 (k<sub><small>1</small></sub>=1.2, 
b=0.75) in our experiments, although any term weighting scheme is possible.
</li>

<li>
<b>Length ratio:</b> The ratio of sentence lengths (i.e., number of tokens).
</li>

<li><b>Token translation ratio:</b> The ratio of tokens in <i>f</i>
that have translations in <i>e</i>, and vice versa. It is possible
to enforce a lower probability limit <i>L</i> to the translations of a token 
<i>f</i><sub><small>i</small></sub>, by only considering tokens 
<i>e</i><sub><small>j</small></sub> such that 
<i>t</i>(e<sub><small>j</small></sub>|f<sub><small>i</small></sub>)><i>L</i>.
</li>

<li><b>Uppercase translation ratio:</b>  The ratio of upper-cased entities in 
<i>f</i> that also appear in <i>e</i>. We define an upper-cased entity as a
multi-token span of consecutive upper-cased tokens, such that tokens to the left and
right of the entity are lower-cased.
</li>
</ol>

The pipeline for training and testing a bitext classifier is handled by the
class <code>ivory.lsh.eval.BitextClassifierUtils</code>, which relies on core Ivory classes
to perform tokenization (<code>ivory.core.tokenize.*</code>), represent
token translation probabilities and vocabularies (<code>edu.umd.hooka.*</code>), 
and translate vectors (<code>ivory.core.util.CLIRUtils</code>). 
See documentation for <a href="docs/hooka.html">Hooka</a>
(source code released as part of <a href="http://lintool.github.com/Cloud9">Cloud<sup>9</sup></a>)
to learn more about how token translation tables and vocabularies are represented
and built. See <a href="publications/Ture_etal_SIGIR2011.pdf">our SIGIR 2011 paper</a> for
a detailed description of our CLIR-based translation approach.<p>

<p>Below is the pipeline of how a given sentence pair is classified:</p>

<p><img style="margin-left: 50px;" width="70%" src="images/bitext.png" alt="Maxent Classification Pipeline" /></p>

<p>The maximum-entropy classifier needs to be trained before we can classify any
sentence pair. This is done by running a set of labeled sentence pairs through 
this pipeline, and have the maximum-entropy learner optimize parameters iteratively:</p>

<p><img style="margin-left: 50px;" width="70%" src="images/maxent.png" alt="Maxent Training Pipeline" /></p>

The open-source <a href="http://opennlp.apache.org/">OpenNLP MaxEnt toolkit</a> (v3.0.0) is used to learn and apply maximum-entropy classification.
For training purposes, we sample 1,000 sentence pairs from an available parallel corpus to serve as positive
instances. In order to generate negative instances, we randomly pick 5,000 sentence pairs from possible 
mis-aligned sentence pairs in the corpus (e.g., 7th source-language sentence and 3rd target-language sentence).
We do not use all 999,000 possible negative instances for training in order to keep a reasonable positive-negative
instance ratio and avoid a biased learning process.
<p>

<b>
<p>increasing training data</p>

<p>compare feature sets</p>

<p>vary L = 0,0.1,...,0.5</p>

<p>different domain?</p>
</b>

  </div>



    <!-- Footer
    ================================================== -->
    <footer class="footer">
      <div class="container">
        <p class="pull-right"><a href="#">Back to top</a></p>
        <p>Designed using <a href="http://twitter.github.com/bootstrap/">Bootstrap</a>.</p>
        <p>Code licensed under <a href="http://www.apache.org/licenses/LICENSE-2.0" target="_blank">Apache License v2.0</a>, documentation under <a href="http://creativecommons.org/licenses/by/3.0/">CC BY 3.0</a>.</p>
      </div>
    </footer>

    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="assets/js/jquery.js"></script>
    <script src="assets/js/google-code-prettify/prettify.js"></script>
    <script src="assets/js/bootstrap-transition.js"></script>
    <script src="assets/js/bootstrap-alert.js"></script>
    <script src="assets/js/bootstrap-modal.js"></script>
    <script src="assets/js/bootstrap-dropdown.js"></script>
    <script src="assets/js/bootstrap-scrollspy.js"></script>
    <script src="assets/js/bootstrap-tab.js"></script>
    <script src="assets/js/bootstrap-tooltip.js"></script>
    <script src="assets/js/bootstrap-popover.js"></script>
    <script src="assets/js/bootstrap-button.js"></script>
    <script src="assets/js/bootstrap-collapse.js"></script>
    <script src="assets/js/bootstrap-carousel.js"></script>
    <script src="assets/js/bootstrap-typeahead.js"></script>
    <script src="assets/js/bootstrap-affix.js"></script>

  </body>
</html>

